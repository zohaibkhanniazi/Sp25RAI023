{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVBXOLoVC5j5",
        "outputId": "a073a105-9a3f-4131-d567-e5a708de17fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before adding FamilySize:\n",
            "   SibSp  Parch\n",
            "0      1      0\n",
            "1      1      0\n",
            "2      0      0\n",
            "3      1      0\n",
            "4      0      0\n",
            "\n",
            "After adding FamilySize:\n",
            "   SibSp  Parch  FamilySize\n",
            "0      1      0           2\n",
            "1      1      0           2\n",
            "2      0      0           1\n",
            "3      1      0           2\n",
            "4      0      0           1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Titanic dataset (example: from seaborn or CSV file)\n",
        "# If using seaborn:\n",
        "# import seaborn as sns\n",
        "# df = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Or from a local CSV:\n",
        "df = pd.read_csv('titanic.csv')  # Adjust path as needed\n",
        "\n",
        "# Show first few rows before adding the feature\n",
        "print(\"Before adding FamilySize:\")\n",
        "print(df[['SibSp', 'Parch']].head())\n",
        "\n",
        "# Create FamilySize feature\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "\n",
        "# Show updated DataFrame\n",
        "print(\"\\nAfter adding FamilySize:\")\n",
        "print(df[['SibSp', 'Parch', 'FamilySize']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = pd.read_csv('titanic.csv')  # adjust path as needed\n",
        "\n",
        "# Drop columns with too many missing or not useful (example)\n",
        "df = df.drop(columns=['Cabin', 'Ticket', 'Name', 'PassengerId'])\n",
        "\n",
        "# Create target variable\n",
        "y = df['Survived']\n",
        "X = df.drop(columns='Survived')\n",
        "\n",
        "# Step 1: Create a FunctionTransformer for FamilySize\n",
        "def add_family_size(X):\n",
        "    X = X.copy()\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1\n",
        "    return X\n",
        "\n",
        "# Wrap in FunctionTransformer\n",
        "family_size_transformer = FunctionTransformer(add_family_size, validate=False)\n",
        "\n",
        "# Step 2: Define preprocessing\n",
        "numeric_features = ['Age', 'Fare', 'SibSp', 'Parch', 'FamilySize']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Step 3: Combine everything into a ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Step 4: Full pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('family_size', family_size_transformer),\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "# Step 5: Split data and transform\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline\n",
        "X_train_transformed = pipeline.fit_transform(X_train)\n",
        "X_test_transformed = pipeline.transform(X_test)\n",
        "\n",
        "print(\"Shape of transformed training data:\", X_train_transformed.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF0blt1uDkcj",
        "outputId": "0be790bc-d0ae-4b0d-e84d-06f964beea97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of transformed training data: (712, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = pd.read_csv(\"titanic.csv\")  # Make sure titanic.csv is in your working directory\n",
        "\n",
        "# Drop irrelevant or highly null columns\n",
        "df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True)\n",
        "\n",
        "# Separate target and features\n",
        "y = df['Survived']\n",
        "X = df.drop(columns='Survived')\n",
        "\n",
        "# Step 1: Create a FunctionTransformer to add 'FamilySize'\n",
        "def add_family_size(X_df):\n",
        "    X_df = X_df.copy()\n",
        "    X_df['FamilySize'] = X_df['SibSp'] + X_df['Parch'] + 1\n",
        "    return X_df\n",
        "\n",
        "family_size_transformer = FunctionTransformer(add_family_size, validate=False)\n",
        "\n",
        "# Step 2: Define preprocessing for each type of column\n",
        "numeric_features = ['Age', 'Fare', 'SibSp', 'Parch', 'FamilySize']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Step 3: Create ColumnTransformer\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Step 4: Final pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('add_family_size', family_size_transformer),\n",
        "    ('preprocessing', preprocessor)\n",
        "])\n",
        "\n",
        "# Step 5: Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Fit and transform\n",
        "X_train_transformed = pipeline.fit_transform(X_train)\n",
        "X_test_transformed = pipeline.transform(X_test)\n",
        "\n",
        "# Output shape and preview\n",
        "print(\"Transformed training data shape:\", X_train_transformed.shape)\n",
        "print(\"First row of transformed data:\\n\", X_train_transformed[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7rYjDxVEEKT",
        "outputId": "058db5d4-caf4-489a-bcc1-c9da37c95268"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed training data shape: (712, 13)\n",
            "First row of transformed data:\n",
            " [ 1.25364106 -0.07868358 -0.47072241 -0.47934164 -0.55466613  0.\n",
            "  1.          0.          0.          1.          1.          0.\n",
            "  0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('add_family_size', family_size_transformer),\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "print(\"Accuracy on test set:\", full_pipeline.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg_K5HveEMc-",
        "outputId": "2860c64b-2fc6-4e5d-e236-0f206d37167b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.8268156424581006\n"
          ]
        }
      ]
    }
  ]
}